{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti103/blob/master/session-6/Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Welcome to the programming exercise. This is part of the series of exercises to help you acquire skills in different techniques to fine-tune your model.\n",
    "\n",
    "**You will learn:**\n",
    "- how regularization can be used to avoid overfitting the data\n",
    "- effects of different regularization techniques (e.g. L1/L2) on coefficients\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Regularization Effects\n",
    "\n",
    "We will begin with a short tutorial on regularization based on a very simple generated 'noisy' dataset and examine the effects of regularization on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Import a Data Set and Plot the Data\n",
    "\n",
    "Import the file 'X_Y_Sinusoid_Data.csv' which contains a noisy set of x and y values that corresponds to a function $y = sin(2\\pi x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following two lines if running the notebook from local machine\n",
    "# import os\n",
    "# data_path = ['data']\n",
    "# filepath = os.sep.join(data_path + ['X_Y_Sinusoid_data.csv'])\n",
    "\n",
    "## If running in colab, read the data from url\n",
    "filepath = 'https://raw.githubusercontent.com/nyp-sit/data/master/X_Y_Sinusoid_Data.csv'\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a set of x and y values that corresponds to the ground truth $y = sin(2\\pi x)$ and plot the sparse data (`x` vs `y`) and the calculated (\"real\") data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "\n",
    "Generate 100 equally spaced x data points over the range of 0 to 1. Using these points, calculate the y-data which represents the \"ground truth\" (the real function) from the equation: $y = sin(2\\pi x)$\n",
    "\n",
    "***Hint:***\n",
    "- use the np.linspace() to generate the required x values\n",
    "- use the np.sin() for the sine function and np.pi for the constant $\\pi$\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "    \n",
    "```python\n",
    "X_real = np.linspace(0, 1.0, 100)\n",
    "Y_real = np.sin(2 * np.pi * X_real)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "X_real = None \n",
    "Y_real = None\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the 'noisy' data\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "plt.plot(data['x'], data['y'], \"o\", label='data')\n",
    "plt.xlabel(\"x data\", fontsize=18)\n",
    "plt.ylabel(\"y data\", fontsize=18, rotation='1')\n",
    "\n",
    "#plot the real function\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Fit the model with higher-oder polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "\n",
    "Using the [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class from Scikit-learn's preprocessing library, create 20th order polynomial features.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "X_data = data[['x']]\n",
    "Y_data = data['y']\n",
    "\n",
    "degree = 20\n",
    "pf = PolynomialFeatures(degree)\n",
    "X_poly = pf.fit_transform(X_data)\n",
    "\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Extract the X- and Y- data from the dataframe \n",
    "X_data = None\n",
    "Y_data = None \n",
    "\n",
    "# Setup the polynomial features\n",
    "degree = 0\n",
    "pf = None\n",
    "\n",
    "# Create the polynomial features\n",
    "X_poly = None\n",
    "\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# print(X_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_data has the 20 data points. What do you think is the shape of X_poly? Print the X_poly to confirm.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "<p>\n",
    "\n",
    "(20,21). Although we specify degree 20 for the polynomial features, 21 features were generated because of the additional bias term. You can omit the bias term by specifying:\n",
    "    \n",
    "```python\n",
    "pf = PolynomialFeatures(degree, include_bias=False)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit this data using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr = lr.fit(X_poly, Y_data)\n",
    "Y_pred = lr.predict(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting predicted value compared to the calculated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='data', alpha=1.0)\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.plot(X_data, Y_pred, marker='^', alpha=0.5, label='pred w/ polynomial features')\n",
    "plt.legend()\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('y data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "\n",
    "What can you observe from the graph about the linear regression model trained with 20th degree polynomial features?\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "<p>\n",
    "The model overfits the data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Use Regularized Model\n",
    "\n",
    "Now we will use the regularized model such as Ridge and Lasso to fit the data with 20th degree polynomial features and observe the difference. \n",
    "\n",
    "***Exercise:***\n",
    "\n",
    "- Perform the regression on the data with polynomial features using ridge regression (\\$\\alpha$=0.001) and lasso regression ($\\alpha$=0.0001). \n",
    "- Plot the results, as was done in section 1.3. \n",
    "\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "    \n",
    "# Fit with ridge regression model\n",
    "ridge = Ridge(alpha=0.0001)\n",
    "ridge = ridge.fit(X_poly, Y_data)\n",
    "Y_pred_ridge = ridge.predict(X_poly)\n",
    "    \n",
    "# Fit with lasso regression model\n",
    "lasso = Lasso(alpha=0.0001)\n",
    "lasso = lasso.fit(X_poly, Y_data)\n",
    "Y_pred_lasso = lasso.predict(X_poly)\n",
    "``` \n",
    "</p>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Fit with ridge regression model\n",
    "ridge = None \n",
    "Y_pred_ridge = None\n",
    "\n",
    "# Similarly, fit the data with lasso regression model\n",
    "\n",
    "lasso = None \n",
    "Y_pred_lasso = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='data')\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.plot(X_data, Y_pred, label='linear regression', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_ridge, label='ridge regression', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_lasso, label='lasso regression', marker='^', alpha=.5)\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the absolute value of coefficients for each model\n",
    "\n",
    "coefficients = pd.DataFrame()\n",
    "coefficients['linear regression'] = lr.coef_\n",
    "coefficients['ridge regression'] = ridge.coef_\n",
    "coefficients['lasso regression'] = lasso.coef_\n",
    "coefficients = coefficients.applymap(abs)\n",
    "\n",
    "coefficients.describe()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***\n",
    "\n",
    "What do you observe about the differences among coefficients of linear regressio, ridge and lasso regression? \n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "\n",
    "The coefficients of non-regularized linear regression are very large, whereas the ridge regression and lasso regression have smaller coefficients.  For lasso too, there are also many coefficients that are 0 (25th percentile have values that are 0). \n",
    "\n",
    "Regularization (L1 or L2) shrinks the sizes of coefficients.\n",
    "\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the magnitude of the coefficients obtained from these regressions, and compare them to those obtained from linear regression in the previous question. The linear regression coefficients is plot using its own y-axis due to their much larger magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "# Setup the dual y-axes\n",
    "ax1 = plt.axes()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the linear regression data\n",
    "ax1.plot(lr.coef_, \n",
    "         color=colors[0], marker='o', label='linear regression')\n",
    "\n",
    "# Plot the regularization data sets\n",
    "ax2.plot(ridge.coef_, \n",
    "         color=colors[1], marker='o', label='ridge regression')\n",
    "\n",
    "ax2.plot(lasso.coef_, \n",
    "         color=colors[2], marker='o', label='lasso regression')\n",
    "\n",
    "# Customize axes scales\n",
    "ax1.set_ylim(-2e14, 2e14)\n",
    "ax2.set_ylim(-25, 25)\n",
    "\n",
    "# Combine the legends\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1+h2, l1+l2)\n",
    "\n",
    "ax1.set(xlabel='coefficients',ylabel='linear regression')\n",
    "ax2.set(ylabel='ridge and lasso regression')\n",
    "\n",
    "ax1.set_xticks(range(len(lr.coef_)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Applying Regularization on Boston housing prices dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sees the effects of L1/L2 regularization on the coefficients of linear model, we will now apply them on real dataset, the Boston housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset (this dataset has already been scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_scaled_boston_data\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "X, y = load_scaled_boston_data()\n",
    "\n",
    "# Split the data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "print(\"Number of features in the datase: {}\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the $R^2$ score (for easier comparison) of linear regression for both train and test score too see if there is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the scores with L2-regularized version using Ridge Regression, using different regularization strength (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge1 = Ridge(alpha=1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge1.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercises:***\n",
    "\n",
    "- How does regularization strength affects the bias and variance of the model? \n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "<p>\n",
    "    \n",
    "In general, by increasing the regularization (increase values of alpha), we can see that the bias increases while the variance decreases.\n",
    "    \n",
    "</p>\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect visually the coefficients of both linear regression and ridge regression with different regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(ridge1.coef_, 's', label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.hlines(0, 0, len(lr.coef_))\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see that coefficients for non-regularized linear regression are much larger than regularized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare Linear Regression with Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercises:***\n",
    "\n",
    "- Now fit the data using Lasso, with different alpha values \\[0.0001, 0.01, 1.0 (default), 10\\]\n",
    "- For each lasso model, print the training set score, test set score and also number of coefficients not zero\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "lasso00001 = Lasso(alpha=0.0001).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\n",
    "\n",
    "\n",
    "lasso001 = Lasso(alpha=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\n",
    "    \n",
    "lasso1 = Lasso(alpha=1.0).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "    \n",
    "\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the data using Lasso, with different alpha values \\[0.0001, 0.01, 1.0 (default), 10\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso with alpha = 0.0001\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso with alpha = 0.01\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso with alpha = 1\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***\n",
    "\n",
    "What happen to bias and variance when you increase the alpha of Lasso regression? Why is that so?\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "<p>\n",
    "The bias has gone up and the variance has comed down. The last model with alpha=1 has such a high bias because Lasso drives most of the coefficients to 0 and left with 3 features. This is too simple a model and thus is not able to fit the training data well, thus shown high bias.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***\n",
    "\n",
    "Display the absolute values (ignore the negative sign) of the coefficients in a dataframe (*Hint*: refer to section 1.4 above for codes). Your display should like this: \n",
    "\n",
    "<img src='images/lasso_coeffs.png' width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "coefficients = pd.DataFrame()\n",
    "coefficients['linear regression'] = lr.coef_ \n",
    "coefficients['lasso alpha=0.0001'] = lasso00001.coef_ \n",
    "coefficients['lasso alpha=0.001'] = lasso001.coef_ \n",
    "coefficients['lasso alpha=1.0'] = lasso1.coef_\n",
    "coefficients = coefficients.applymap(abs)\n",
    "coefficients.describe()  \n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***\n",
    "\n",
    "Now plot the valuesof the coefficients of linear regression and lasso with different alphas. \n",
    "What can you conclude about the sizes of the coefficients?\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(lr.coef_, 'o', label=\"linear regression\")\n",
    "plt.plot(lasso00001.coef_, 's', label=\"Lasso alpha=0.0001\")\n",
    "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "plt.plot(lasso1.coef_, 'v', label=\"Lasso alpha=1\")\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
